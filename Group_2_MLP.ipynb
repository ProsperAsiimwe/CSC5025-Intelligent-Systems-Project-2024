{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv('JSE_clean_truncated.csv')\n",
        "\n",
        "# Normalize the data (MinMax scaling)\n",
        "scaler = MinMaxScaler()\n",
        "data_normalized = scaler.fit_transform(data.values)\n",
        "\n",
        "# Sliding window creation\n",
        "def create_sliding_window(data, input_window, output_window):\n",
        "    x, y = [], []\n",
        "    for i in range(len(data) - input_window - output_window):\n",
        "        x.append(data[i:(i + input_window)])\n",
        "        y.append(data[(i + input_window):(i + input_window + output_window)])\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "# MLP Model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # No activation on the output layer\n",
        "        return x\n",
        "\n",
        "# Training and evaluation loop with logging and plotting\n",
        "def train_and_evaluate_model(hyperparameters, run_id, share_name):\n",
        "    # Extract hyperparameters\n",
        "    input_window = hyperparameters['input_window']\n",
        "    output_window = hyperparameters['output_window']\n",
        "    hidden_nodes = hyperparameters['hidden_nodes']\n",
        "    learning_rate = hyperparameters['learning_rate']\n",
        "    batch_size = hyperparameters['batch_size']\n",
        "    epochs = hyperparameters['epochs']\n",
        "\n",
        "    # Create sliding window\n",
        "    x, y = create_sliding_window(data_normalized, input_window, output_window)\n",
        "\n",
        "    # Split data (7:2:1 train/val/test)\n",
        "    num_samples = x.shape[0]\n",
        "    num_train = int(0.7 * num_samples)\n",
        "    num_val = int(0.2 * num_samples)\n",
        "    num_test = num_samples - num_train - num_val\n",
        "\n",
        "    x_train, y_train = x[:num_train], y[:num_train]\n",
        "    x_val, y_val = x[num_train:num_train + num_val], y[num_train:num_train + num_val]\n",
        "    x_test, y_test = x[num_train + num_val:], y[num_train + num_val:]\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    train_dataset = TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train))\n",
        "    valid_dataset = TensorDataset(torch.Tensor(x_val), torch.Tensor(y_val))\n",
        "    test_dataset = TensorDataset(torch.Tensor(x_test), torch.Tensor(y_test))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize the model\n",
        "    input_dim = input_window * data.shape[1]  # Adjust input size based on number of stocks (columns)\n",
        "    output_dim = output_window * data.shape[1]  # Adjust for output dimensions\n",
        "\n",
        "    model = MLP(input_size=input_dim, hidden_size=hidden_nodes, output_size=output_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Lists to store losses for plotting\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "\n",
        "    # Start training and evaluation\n",
        "    log_lines = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        model.train()\n",
        "        for x, y in train_loader:\n",
        "            x = x.view(x.size(0), -1)\n",
        "            y = y.view(y.size(0), -1)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x)\n",
        "            loss = criterion(output, y)\n",
        "            train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        valid_loss = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, y in valid_loader:\n",
        "                x = x.view(x.size(0), -1)\n",
        "                y = y.view(y.size(0), -1)\n",
        "                output = model(x)\n",
        "                loss = criterion(output, y)\n",
        "                valid_loss += loss.item()\n",
        "        avg_valid_loss = valid_loss / len(valid_loader)\n",
        "        valid_losses.append(avg_valid_loss)\n",
        "        log_lines.append(f'Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_valid_loss:.4f}')\n",
        "\n",
        "    # Testing\n",
        "    test_loss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x = x.view(x.size(0), -1)\n",
        "            y = y.view(y.size(0), -1)\n",
        "            output = model(x)\n",
        "            loss = criterion(output, y)\n",
        "            test_loss += loss.item()\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    # Log the results\n",
        "    elapsed_time = time.time() - start_time\n",
        "    log_lines.append(f'Test Loss: {avg_test_loss:.4f}')\n",
        "    log_lines.append(f'Training Time: {elapsed_time:.2f} seconds')\n",
        "    log_lines.append(f'Hyperparameters: {hyperparameters}')\n",
        "    log_lines.append('---' * 10)\n",
        "\n",
        "    # Save logs to file\n",
        "    with open(f'logss_{share_name}.txt', 'a') as f:\n",
        "        for line in log_lines:\n",
        "            f.write(line + '\\n')\n",
        "\n",
        "    # Print final metrics for easy comparison\n",
        "    print(f\"Run {run_id} ({share_name}) - Final Test Loss: {avg_test_loss:.4f}, Training Time: {elapsed_time:.2f}s\")\n",
        "\n",
        "    # Plot training and validation losses\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, epochs + 1), valid_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'Training and Validation Loss (Run {run_id}, {share_name})')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'loss2_plot_run_{run_id}_{share_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "# Define hyperparameter configurations to test\n",
        "hyperparameter_configurations = [\n",
        "    {'input_window': 30, 'output_window': 1, 'hidden_nodes': 32, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 50},\n",
        "    {'input_window': 60, 'output_window': 1, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 64, 'epochs': 100},\n",
        "    {'input_window': 30, 'output_window': 2, 'hidden_nodes': 128, 'learning_rate': 0.0005, 'batch_size': 16, 'epochs': 100},\n",
        "    # Add more configurations if needed\n",
        "]\n",
        "\n",
        "# Get the list of share names from the dataset for iteration\n",
        "share_names = data.columns\n",
        "\n",
        "# Run experiments with each configuration on each share\n",
        "for share_name in share_names:\n",
        "    for i, config in enumerate(hyperparameter_configurations, 1):\n",
        "        train_and_evaluate_model(config, run_id=i, share_name=share_name)\n",
        "\n",
        "print(\"All experiments completed, results logged in log files, and performance graphs saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EZMuufpGWe4",
        "outputId": "49e3acfa-2f99-4564-b1f1-51ab5bb52b9a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 1 (ASPEN) - Final Test Loss: 0.0590, Training Time: 7.17s\n",
            "Run 2 (ASPEN) - Final Test Loss: 0.0767, Training Time: 14.36s\n",
            "Run 3 (ASPEN) - Final Test Loss: 0.0430, Training Time: 39.16s\n",
            "Run 1 (CAPITEC) - Final Test Loss: 0.0672, Training Time: 6.25s\n",
            "Run 2 (CAPITEC) - Final Test Loss: 0.0514, Training Time: 16.92s\n",
            "Run 3 (CAPITEC) - Final Test Loss: 0.0478, Training Time: 43.50s\n",
            "Run 1 (IMPLATS) - Final Test Loss: 0.0493, Training Time: 7.06s\n",
            "Run 2 (IMPLATS) - Final Test Loss: 0.0525, Training Time: 15.61s\n",
            "Run 3 (IMPLATS) - Final Test Loss: 0.0491, Training Time: 41.99s\n",
            "Run 1 (GROWPNT) - Final Test Loss: 0.0652, Training Time: 7.25s\n",
            "Run 2 (GROWPNT) - Final Test Loss: 0.0766, Training Time: 14.89s\n",
            "Run 3 (GROWPNT) - Final Test Loss: 0.0537, Training Time: 40.99s\n",
            "Run 1 (NORTHAM) - Final Test Loss: 0.0692, Training Time: 6.86s\n",
            "Run 2 (NORTHAM) - Final Test Loss: 0.0477, Training Time: 14.92s\n",
            "Run 3 (NORTHAM) - Final Test Loss: 0.0583, Training Time: 42.85s\n",
            "Run 1 (ANGGOLD) - Final Test Loss: 0.0557, Training Time: 7.09s\n",
            "Run 2 (ANGGOLD) - Final Test Loss: 0.0640, Training Time: 14.07s\n",
            "Run 3 (ANGGOLD) - Final Test Loss: 0.0569, Training Time: 41.55s\n",
            "Run 1 (BATS) - Final Test Loss: 0.0532, Training Time: 6.56s\n",
            "Run 2 (BATS) - Final Test Loss: 0.0547, Training Time: 15.48s\n",
            "Run 3 (BATS) - Final Test Loss: 0.0514, Training Time: 42.99s\n",
            "Run 1 (EXXARO) - Final Test Loss: 0.0958, Training Time: 6.44s\n",
            "Run 2 (EXXARO) - Final Test Loss: 0.0430, Training Time: 15.04s\n",
            "Run 3 (EXXARO) - Final Test Loss: 0.0315, Training Time: 42.67s\n",
            "Run 1 (WOOLIES) - Final Test Loss: 0.0606, Training Time: 6.62s\n",
            "Run 2 (WOOLIES) - Final Test Loss: 0.0615, Training Time: 14.38s\n",
            "Run 3 (WOOLIES) - Final Test Loss: 0.0310, Training Time: 44.38s\n",
            "Run 1 (NASPERS-N-) - Final Test Loss: 0.0634, Training Time: 7.09s\n",
            "Run 2 (NASPERS-N-) - Final Test Loss: 0.0669, Training Time: 13.83s\n",
            "Run 3 (NASPERS-N-) - Final Test Loss: 0.0434, Training Time: 42.15s\n",
            "Run 1 (CLICKS) - Final Test Loss: 0.0712, Training Time: 6.27s\n",
            "Run 2 (CLICKS) - Final Test Loss: 0.0537, Training Time: 14.30s\n",
            "Run 3 (CLICKS) - Final Test Loss: 0.0602, Training Time: 41.55s\n",
            "Run 1 (BIDVEST) - Final Test Loss: 0.0636, Training Time: 6.11s\n",
            "Run 2 (BIDVEST) - Final Test Loss: 0.0580, Training Time: 14.72s\n",
            "Run 3 (BIDVEST) - Final Test Loss: 0.0356, Training Time: 42.21s\n",
            "Run 1 (SANLAM) - Final Test Loss: 0.0653, Training Time: 6.27s\n",
            "Run 2 (SANLAM) - Final Test Loss: 0.0490, Training Time: 13.81s\n",
            "Run 3 (SANLAM) - Final Test Loss: 0.0455, Training Time: 39.16s\n",
            "Run 1 (REMGRO) - Final Test Loss: 0.0775, Training Time: 6.95s\n",
            "Run 2 (REMGRO) - Final Test Loss: 0.0456, Training Time: 13.78s\n",
            "Run 3 (REMGRO) - Final Test Loss: 0.0479, Training Time: 41.91s\n",
            "Run 1 (GFIELDS) - Final Test Loss: 0.0539, Training Time: 6.98s\n",
            "Run 2 (GFIELDS) - Final Test Loss: 0.0484, Training Time: 13.79s\n",
            "Run 3 (GFIELDS) - Final Test Loss: 0.0456, Training Time: 40.87s\n",
            "Run 1 (RICHEMONT) - Final Test Loss: 0.0743, Training Time: 7.15s\n",
            "Run 2 (RICHEMONT) - Final Test Loss: 0.0526, Training Time: 13.91s\n",
            "Run 3 (RICHEMONT) - Final Test Loss: 0.0437, Training Time: 41.41s\n",
            "Run 1 (STANBANK) - Final Test Loss: 0.0578, Training Time: 6.37s\n",
            "Run 2 (STANBANK) - Final Test Loss: 0.0453, Training Time: 14.22s\n",
            "Run 3 (STANBANK) - Final Test Loss: 0.0495, Training Time: 40.60s\n",
            "Run 1 (SHOPRIT) - Final Test Loss: 0.0707, Training Time: 6.17s\n",
            "Run 2 (SHOPRIT) - Final Test Loss: 0.0598, Training Time: 13.73s\n",
            "Run 3 (SHOPRIT) - Final Test Loss: 0.0425, Training Time: 43.23s\n",
            "Run 1 (INVLTD) - Final Test Loss: 0.0513, Training Time: 6.26s\n",
            "Run 2 (INVLTD) - Final Test Loss: 0.0366, Training Time: 13.88s\n",
            "Run 3 (INVLTD) - Final Test Loss: 0.0370, Training Time: 40.45s\n",
            "Run 1 (MONDIPLC) - Final Test Loss: 0.0627, Training Time: 6.83s\n",
            "Run 2 (MONDIPLC) - Final Test Loss: 0.0480, Training Time: 14.31s\n",
            "Run 3 (MONDIPLC) - Final Test Loss: 0.0523, Training Time: 41.85s\n",
            "Run 1 (INVPLC) - Final Test Loss: 0.0571, Training Time: 7.16s\n",
            "Run 2 (INVPLC) - Final Test Loss: 0.0555, Training Time: 13.90s\n",
            "Run 3 (INVPLC) - Final Test Loss: 0.0461, Training Time: 42.43s\n",
            "Run 1 (DISCOVERY) - Final Test Loss: 0.0711, Training Time: 7.06s\n",
            "Run 2 (DISCOVERY) - Final Test Loss: 0.0708, Training Time: 14.55s\n",
            "Run 3 (DISCOVERY) - Final Test Loss: 0.0324, Training Time: 40.90s\n",
            "Run 1 (AMPLATS) - Final Test Loss: 0.0633, Training Time: 6.81s\n",
            "Run 2 (AMPLATS) - Final Test Loss: 0.0620, Training Time: 13.73s\n",
            "Run 3 (AMPLATS) - Final Test Loss: 0.0516, Training Time: 41.63s\n",
            "Run 1 (ANGLO) - Final Test Loss: 0.0594, Training Time: 6.35s\n",
            "Run 2 (ANGLO) - Final Test Loss: 0.0698, Training Time: 14.15s\n",
            "Run 3 (ANGLO) - Final Test Loss: 0.0346, Training Time: 41.15s\n",
            "Run 1 (FIRSTRAND) - Final Test Loss: 0.0498, Training Time: 6.06s\n",
            "Run 2 (FIRSTRAND) - Final Test Loss: 0.0487, Training Time: 13.15s\n",
            "Run 3 (FIRSTRAND) - Final Test Loss: 0.0359, Training Time: 39.56s\n",
            "Run 1 (NEDBANK) - Final Test Loss: 0.0617, Training Time: 8.11s\n",
            "Run 2 (NEDBANK) - Final Test Loss: 0.0625, Training Time: 14.06s\n",
            "Run 3 (NEDBANK) - Final Test Loss: 0.0424, Training Time: 42.86s\n",
            "Run 1 (SASOL) - Final Test Loss: 0.0645, Training Time: 6.68s\n",
            "Run 2 (SASOL) - Final Test Loss: 0.0444, Training Time: 13.65s\n",
            "Run 3 (SASOL) - Final Test Loss: 0.0441, Training Time: 41.44s\n",
            "Run 1 (SPAR) - Final Test Loss: 0.0556, Training Time: 7.01s\n",
            "Run 2 (SPAR) - Final Test Loss: 0.0820, Training Time: 13.58s\n",
            "Run 3 (SPAR) - Final Test Loss: 0.0458, Training Time: 41.90s\n",
            "Run 1 (VODACOM) - Final Test Loss: 0.0823, Training Time: 7.08s\n",
            "Run 2 (VODACOM) - Final Test Loss: 0.0667, Training Time: 13.94s\n",
            "Run 3 (VODACOM) - Final Test Loss: 0.0466, Training Time: 40.64s\n",
            "Run 1 (MTN_GROUP) - Final Test Loss: 0.0574, Training Time: 6.54s\n",
            "Run 2 (MTN_GROUP) - Final Test Loss: 0.0403, Training Time: 14.30s\n",
            "Run 3 (MTN_GROUP) - Final Test Loss: 0.0469, Training Time: 43.14s\n",
            "All experiments completed, results logged in log files, and performance graphs saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "weAlaOuD_UEc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('JSE_clean_truncated.csv')\n",
        "\n",
        "# Normalize the data (MinMax scaling)\n",
        "scaler = MinMaxScaler()\n",
        "data_normalized = scaler.fit_transform(data.values)\n",
        "\n",
        "# Create sliding window\n",
        "def create_sliding_window(data, input_window, output_window):\n",
        "    x, y = [], []\n",
        "    for i in range(len(data) - input_window - output_window):\n",
        "        x.append(data[i:(i + input_window)])\n",
        "        y.append(data[(i + input_window):(i + input_window + output_window)])\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "# MLP Model with Batch Normalization and Dropout\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout = nn.Dropout(0.3)  # Dropout rate\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.fc1(x)))  # BatchNorm + ReLU\n",
        "        x = self.dropout(x)  # Dropout\n",
        "        x = self.fc2(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# Huber loss function\n",
        "def get_loss_function():\n",
        "    return nn.HuberLoss()\n",
        "\n",
        "# Manual metrics calculation\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    epsilon = 1e-10  # A small constant to avoid division by zero\n",
        "    mape = torch.mean(torch.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "    mae = torch.mean(torch.abs(y_true - y_pred))\n",
        "    rmse = torch.sqrt(torch.mean((y_true - y_pred) ** 2))\n",
        "\n",
        "    return {\n",
        "        'MAPE': mape.item(),\n",
        "        'MAE': mae.item(),\n",
        "        'RMSE': rmse.item(),\n",
        "    }\n",
        "\n",
        "# Training and evaluation loop with logging and plotting\n",
        "def train_and_evaluate_model(hyperparameters, run_id, csv_writer):\n",
        "    # Extract hyperparameters\n",
        "    input_window = hyperparameters['input_window']\n",
        "    output_window = hyperparameters['output_window']\n",
        "    hidden_nodes = hyperparameters['hidden_nodes']\n",
        "    learning_rate = hyperparameters['learning_rate']\n",
        "    batch_size = hyperparameters['batch_size']\n",
        "    epochs = hyperparameters['epochs']\n",
        "\n",
        "    # Create sliding window\n",
        "    x, y = create_sliding_window(data_normalized, input_window, output_window)\n",
        "\n",
        "    # Split data (60% train, 20% validation, 20% test to preserve temporal order)\n",
        "    num_samples = x.shape[0]\n",
        "    num_train = int(0.6 * num_samples)\n",
        "    num_val = int(0.2 * num_samples)\n",
        "    num_test = num_samples - num_train - num_val\n",
        "\n",
        "    x_train, y_train = x[:num_train], y[:num_train]\n",
        "    x_val, y_val = x[num_train:num_train + num_val], y[num_train:num_train + num_val]\n",
        "    x_test, y_test = x[num_train + num_val:], y[num_train + num_val:]\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    train_dataset = TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train))\n",
        "    valid_dataset = TensorDataset(torch.Tensor(x_val), torch.Tensor(y_val))\n",
        "    test_dataset = TensorDataset(torch.Tensor(x_test), torch.Tensor(y_test))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    # Initialize the model\n",
        "    input_dim = input_window * data.shape[1]\n",
        "    output_dim = output_window * data.shape[1]\n",
        "\n",
        "    model = MLP(input_size=input_dim, hidden_size=hidden_nodes, output_size=output_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
        "    criterion = get_loss_function()\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_val_loss = float('inf')\n",
        "    patience, wait = 10, 0\n",
        "\n",
        "    # Lists to store losses for plotting\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    metrics_store = []  # Store metrics for test evaluation\n",
        "\n",
        "    # Start training and evaluation\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        model.train()\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch = x_batch.view(x_batch.size(0), -1)\n",
        "            y_batch = y_batch.view(y_batch.size(0), -1)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        valid_loss = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in valid_loader:\n",
        "                x_batch = x_batch.view(x_batch.size(0), -1)\n",
        "                y_batch = y_batch.view(y_batch.size(0), -1)\n",
        "                output = model(x_batch)\n",
        "                loss = criterion(output, y_batch)\n",
        "                valid_loss += loss.item()\n",
        "        avg_valid_loss = valid_loss / len(valid_loader)\n",
        "        valid_losses.append(avg_valid_loss)\n",
        "\n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step(avg_valid_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if avg_valid_loss < best_val_loss:\n",
        "            best_val_loss = avg_valid_loss\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Testing\n",
        "    test_loss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            x_batch = x_batch.view(x_batch.size(0), -1)\n",
        "            y_batch = y_batch.view(y_batch.size(0), -1)\n",
        "            output = model(x_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            test_loss += loss.item()\n",
        "            # Calculate performance metrics\n",
        "            metrics = calculate_metrics(y_batch, output)\n",
        "            metrics_store.append(metrics)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Write to CSV file\n",
        "    for metrics in metrics_store:\n",
        "        csv_writer.writerow([run_id, input_window, output_window, test_loss, metrics['MAPE'], metrics['MAE'], metrics['RMSE'], elapsed_time])\n",
        "\n",
        "    # Generate plots\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, len(valid_losses) + 1), valid_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'Training and Validation Loss (Run {run_id}, Input Window: {input_window}, Horizon: {output_window})')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'loss_plot_run_{run_id}.png')\n",
        "    plt.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DZRRBSp-bpo3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter configurations for all horizons and input windows\n",
        "hyperparameter_configurations = [\n",
        "    # Input Window 30\n",
        "    {'input_window': 30, 'output_window': 1, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "    {'input_window': 30, 'output_window': 2, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "    {'input_window': 30, 'output_window': 5, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "    {'input_window': 30, 'output_window': 10, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "    {'input_window': 30, 'output_window': 30, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "\n",
        "    # Input Window 60\n",
        "    {'input_window': 60, 'output_window': 1, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "    {'input_window': 60, 'output_window': 2, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "    {'input_window': 60, 'output_window': 5, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "    {'input_window': 60, 'output_window': 10, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "    {'input_window': 60, 'output_window': 30, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "\n",
        "    # Input Window 120\n",
        "    {'input_window': 120, 'output_window': 1, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "    {'input_window': 120, 'output_window': 2, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "    {'input_window': 120, 'output_window': 5, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "    {'input_window': 120, 'output_window': 10, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "    {'input_window': 120, 'output_window': 30, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "]\n",
        "# Run experiments with each configuration and save to CSV\n",
        "with open('performance_log.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Run', 'Input Window', 'Horizon', 'Test Loss', 'MAPE', 'MAE', 'RMSE', 'Training Time'])\n",
        "\n",
        "    for i, config in enumerate(hyperparameter_configurations, 1):\n",
        "        train_and_evaluate_model(config, run_id=i, csv_writer=writer)\n",
        "\n",
        "print(\"All experiments completed, results logged in performance_log.csv, and performance graphs saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfqlR02Wdg-3",
        "outputId": "b223fc7c-706c-47e4-fe89-3fae487fd766"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 22\n",
            "Early stopping at epoch 19\n",
            "Early stopping at epoch 41\n",
            "Early stopping at epoch 19\n",
            "Early stopping at epoch 11\n",
            "Early stopping at epoch 23\n",
            "Early stopping at epoch 25\n",
            "Early stopping at epoch 24\n",
            "Early stopping at epoch 28\n",
            "Early stopping at epoch 36\n",
            "Early stopping at epoch 26\n",
            "Early stopping at epoch 37\n",
            "Early stopping at epoch 39\n",
            "Early stopping at epoch 25\n",
            "Early stopping at epoch 41\n",
            "All experiments completed, results logged in performance_log.csv, and performance graphs saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('JSE_clean_truncated.csv')\n",
        "\n",
        "# Normalize the data (MinMax scaling)\n",
        "scaler = MinMaxScaler()\n",
        "data_normalized = scaler.fit_transform(data.values)\n",
        "\n",
        "# Create sliding window\n",
        "def create_sliding_window(data, input_window, output_window):\n",
        "    x, y = [], []\n",
        "    for i in range(len(data) - input_window - output_window):\n",
        "        x.append(data[i:(i + input_window)])\n",
        "        y.append(data[(i + input_window):(i + input_window + output_window)])\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "# MLP Model with Batch Normalization and Dropout\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout = nn.Dropout(0.3)  # Dropout rate\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.fc1(x)))  # BatchNorm + ReLU\n",
        "        x = self.dropout(x)  # Dropout\n",
        "        x = self.fc2(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# Huber loss function\n",
        "def get_loss_function():\n",
        "    return nn.HuberLoss()\n",
        "\n",
        "# Manual metrics calculation\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    epsilon = 1e-10  # A small constant to avoid division by zero\n",
        "    mape = torch.mean(torch.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "    mae = torch.mean(torch.abs(y_true - y_pred))\n",
        "    rmse = torch.sqrt(torch.mean((y_true - y_pred) ** 2))\n",
        "\n",
        "    return {\n",
        "        'MAPE': mape.item(),\n",
        "        'MAE': mae.item(),\n",
        "        'RMSE': rmse.item(),\n",
        "    }\n",
        "\n",
        "# Training and evaluation loop with logging and plotting\n",
        "def train_and_evaluate_model(hyperparameters, run_id, csv_writer):\n",
        "    # Extract hyperparameters\n",
        "    input_window = hyperparameters['input_window']\n",
        "    output_window = hyperparameters['output_window']\n",
        "    hidden_nodes = hyperparameters['hidden_nodes']\n",
        "    learning_rate = hyperparameters['learning_rate']\n",
        "    batch_size = hyperparameters['batch_size']\n",
        "    epochs = hyperparameters['epochs']\n",
        "    weight_decay = hyperparameters.get('weight_decay', 0.0)  # Add weight decay\n",
        "\n",
        "    # Sliding window creation and data split remains the same\n",
        "\n",
        "\n",
        "\n",
        "    # Create sliding window\n",
        "    x, y = create_sliding_window(data_normalized, input_window, output_window)\n",
        "\n",
        "    # Split data (60% train, 20% validation, 20% test to preserve temporal order)\n",
        "    num_samples = x.shape[0]\n",
        "    num_train = int(0.6 * num_samples)\n",
        "    num_val = int(0.2 * num_samples)\n",
        "    num_test = num_samples - num_train - num_val\n",
        "\n",
        "    x_train, y_train = x[:num_train], y[:num_train]\n",
        "    x_val, y_val = x[num_train:num_train + num_val], y[num_train:num_train + num_val]\n",
        "    x_test, y_test = x[num_train + num_val:], y[num_train + num_val:]\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    train_dataset = TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train))\n",
        "    valid_dataset = TensorDataset(torch.Tensor(x_val), torch.Tensor(y_val))\n",
        "    test_dataset = TensorDataset(torch.Tensor(x_test), torch.Tensor(y_test))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    # Initialize the model\n",
        "    input_dim = input_window * data.shape[1]\n",
        "    output_dim = output_window * data.shape[1]\n",
        "\n",
        "    model = MLP(input_size=input_dim, hidden_size=hidden_nodes, output_size=output_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)  # Apply weight decay\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
        "    criterion = get_loss_function()\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_val_loss = float('inf')\n",
        "    patience, wait = 10, 0\n",
        "\n",
        "    # Lists to store losses for plotting\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    metrics_store = []  # Store metrics for test evaluation\n",
        "\n",
        "    # Start training and evaluation\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        model.train()\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch = x_batch.view(x_batch.size(0), -1)\n",
        "            y_batch = y_batch.view(y_batch.size(0), -1)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        valid_loss = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in valid_loader:\n",
        "                x_batch = x_batch.view(x_batch.size(0), -1)\n",
        "                y_batch = y_batch.view(y_batch.size(0), -1)\n",
        "                output = model(x_batch)\n",
        "                loss = criterion(output, y_batch)\n",
        "                valid_loss += loss.item()\n",
        "        avg_valid_loss = valid_loss / len(valid_loader)\n",
        "        valid_losses.append(avg_valid_loss)\n",
        "\n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step(avg_valid_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if avg_valid_loss < best_val_loss:\n",
        "            best_val_loss = avg_valid_loss\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Testing\n",
        "    test_loss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            x_batch = x_batch.view(x_batch.size(0), -1)\n",
        "            y_batch = y_batch.view(y_batch.size(0), -1)\n",
        "            output = model(x_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            test_loss += loss.item()\n",
        "            # Calculate performance metrics\n",
        "            metrics = calculate_metrics(y_batch, output)\n",
        "            metrics_store.append(metrics)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Write to CSV file\n",
        "    for metrics in metrics_store:\n",
        "        csv_writer.writerow([run_id, input_window, output_window, test_loss, metrics['MAPE'], metrics['MAE'], metrics['RMSE'], elapsed_time])\n",
        "\n",
        "    # Generate plots\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, len(valid_losses) + 1), valid_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'Training and Validation Loss (Run {run_id}, Input Window: {input_window}, Horizon: {output_window})')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'loss_plot_run_{run_id}.png')\n",
        "    plt.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gM7taxR-dh6N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter_configurations = [\n",
        "    {'input_window': 30, 'output_window': 1, 'hidden_nodes': 64, 'learning_rate': 0.0005, 'batch_size': 32, 'epochs': 50, 'weight_decay': 0.01},\n",
        "    {'input_window': 60, 'output_window': 2, 'hidden_nodes': 128, 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 100, 'weight_decay': 0.001},\n",
        "    # Add other configurations to explore\n",
        "]\n",
        "\n",
        "# Run experiments with each configuration and save to CSV\n",
        "with open('performance_log.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Run', 'Input Window', 'Horizon', 'Test Loss', 'MAPE', 'MAE', 'RMSE', 'Training Time'])\n",
        "\n",
        "    for i, config in enumerate(hyperparameter_configurations, 1):\n",
        "        train_and_evaluate_model(config, run_id=i, csv_writer=writer)\n",
        "\n",
        "print(\"All experiments completed, results logged in performance_log.csv, and performance graphs saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cy_9pGhsCMx",
        "outputId": "4943f659-e354-47cd-fc81-8adcf1b09156"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 15\n",
            "Early stopping at epoch 23\n",
            "All experiments completed, results logged in performance_log.csv, and performance graphs saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b38hjB9ZsIeH"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}