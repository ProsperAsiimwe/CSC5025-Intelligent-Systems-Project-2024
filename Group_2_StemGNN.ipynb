{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8pEbHJjW_XrE",
        "outputId": "0293bf59-e178-48a7-b5ba-b82118c064f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32, 30])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([2, 30])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([18, 30])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([18, 1])) that is different to the input size (torch.Size([18, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([18, 1])) that is different to the input size (torch.Size([18, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Train Loss: 0.0530, Val Loss: 0.0633, MAPE: inf%, MAE: 0.2115, RMSE: 0.2499\n",
            "Epoch [2/50], Train Loss: 0.0454, Val Loss: 0.0629, MAPE: inf%, MAE: 0.2090, RMSE: 0.2491\n",
            "Epoch [3/50], Train Loss: 0.0451, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2049, RMSE: 0.2487\n",
            "Epoch [4/50], Train Loss: 0.0452, Val Loss: 0.0628, MAPE: inf%, MAE: 0.2038, RMSE: 0.2489\n",
            "Epoch [5/50], Train Loss: 0.0452, Val Loss: 0.0628, MAPE: inf%, MAE: 0.2084, RMSE: 0.2489\n",
            "Epoch [6/50], Train Loss: 0.0450, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2052, RMSE: 0.2487\n",
            "Epoch [7/50], Train Loss: 0.0455, Val Loss: 0.0632, MAPE: inf%, MAE: 0.2018, RMSE: 0.2498\n",
            "Epoch [8/50], Train Loss: 0.0453, Val Loss: 0.0630, MAPE: inf%, MAE: 0.2025, RMSE: 0.2494\n",
            "Epoch [9/50], Train Loss: 0.0453, Val Loss: 0.0628, MAPE: inf%, MAE: 0.2081, RMSE: 0.2489\n",
            "Epoch [10/50], Train Loss: 0.0450, Val Loss: 0.0630, MAPE: inf%, MAE: 0.2026, RMSE: 0.2493\n",
            "Epoch [11/50], Train Loss: 0.0451, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2057, RMSE: 0.2486\n",
            "Epoch [12/50], Train Loss: 0.0449, Val Loss: 0.0635, MAPE: inf%, MAE: 0.2125, RMSE: 0.2502\n",
            "Epoch [13/50], Train Loss: 0.0450, Val Loss: 0.0628, MAPE: inf%, MAE: 0.2080, RMSE: 0.2489\n",
            "Epoch [14/50], Train Loss: 0.0450, Val Loss: 0.0628, MAPE: inf%, MAE: 0.2036, RMSE: 0.2489\n",
            "Epoch [15/50], Train Loss: 0.0449, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2059, RMSE: 0.2486\n",
            "Epoch [16/50], Train Loss: 0.0450, Val Loss: 0.0629, MAPE: inf%, MAE: 0.2027, RMSE: 0.2492\n",
            "Epoch [17/50], Train Loss: 0.0458, Val Loss: 0.0658, MAPE: inf%, MAE: 0.2211, RMSE: 0.2549\n",
            "Epoch [18/50], Train Loss: 0.0453, Val Loss: 0.0628, MAPE: inf%, MAE: 0.2085, RMSE: 0.2489\n",
            "Epoch [19/50], Train Loss: 0.0450, Val Loss: 0.0632, MAPE: inf%, MAE: 0.2114, RMSE: 0.2497\n",
            "Epoch [20/50], Train Loss: 0.0450, Val Loss: 0.0628, MAPE: inf%, MAE: 0.2087, RMSE: 0.2489\n",
            "Epoch [21/50], Train Loss: 0.0451, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2047, RMSE: 0.2487\n",
            "Epoch [22/50], Train Loss: 0.0451, Val Loss: 0.0626, MAPE: inf%, MAE: 0.2054, RMSE: 0.2486\n",
            "Epoch [23/50], Train Loss: 0.0450, Val Loss: 0.0628, MAPE: inf%, MAE: 0.2038, RMSE: 0.2489\n",
            "Epoch [24/50], Train Loss: 0.0453, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2045, RMSE: 0.2487\n",
            "Epoch [25/50], Train Loss: 0.0456, Val Loss: 0.0642, MAPE: inf%, MAE: 0.2000, RMSE: 0.2517\n",
            "Epoch [26/50], Train Loss: 0.0452, Val Loss: 0.0626, MAPE: inf%, MAE: 0.2055, RMSE: 0.2486\n",
            "Epoch [27/50], Train Loss: 0.0452, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2071, RMSE: 0.2487\n",
            "Epoch [28/50], Train Loss: 0.0452, Val Loss: 0.0626, MAPE: inf%, MAE: 0.2056, RMSE: 0.2486\n",
            "Epoch [29/50], Train Loss: 0.0454, Val Loss: 0.0629, MAPE: inf%, MAE: 0.2028, RMSE: 0.2493\n",
            "Epoch [30/50], Train Loss: 0.0451, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2060, RMSE: 0.2486\n",
            "Epoch [31/50], Train Loss: 0.0451, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2057, RMSE: 0.2486\n",
            "Epoch [32/50], Train Loss: 0.0456, Val Loss: 0.0630, MAPE: inf%, MAE: 0.2025, RMSE: 0.2494\n",
            "Epoch [33/50], Train Loss: 0.0453, Val Loss: 0.0626, MAPE: inf%, MAE: 0.2059, RMSE: 0.2486\n",
            "Epoch [34/50], Train Loss: 0.0452, Val Loss: 0.0667, MAPE: inf%, MAE: 0.2236, RMSE: 0.2566\n",
            "Epoch [35/50], Train Loss: 0.0451, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2078, RMSE: 0.2488\n",
            "Epoch [36/50], Train Loss: 0.0450, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2051, RMSE: 0.2487\n",
            "Epoch [37/50], Train Loss: 0.0453, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2071, RMSE: 0.2487\n",
            "Epoch [38/50], Train Loss: 0.0453, Val Loss: 0.0645, MAPE: inf%, MAE: 0.2165, RMSE: 0.2522\n",
            "Epoch [39/50], Train Loss: 0.0452, Val Loss: 0.0631, MAPE: inf%, MAE: 0.2101, RMSE: 0.2495\n",
            "Epoch [40/50], Train Loss: 0.0453, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2062, RMSE: 0.2487\n",
            "Epoch [41/50], Train Loss: 0.0449, Val Loss: 0.0635, MAPE: inf%, MAE: 0.2122, RMSE: 0.2502\n",
            "Epoch [42/50], Train Loss: 0.0451, Val Loss: 0.0636, MAPE: inf%, MAE: 0.2130, RMSE: 0.2506\n",
            "Epoch [43/50], Train Loss: 0.0452, Val Loss: 0.0632, MAPE: inf%, MAE: 0.2106, RMSE: 0.2496\n",
            "Epoch [44/50], Train Loss: 0.0451, Val Loss: 0.0633, MAPE: inf%, MAE: 0.2117, RMSE: 0.2500\n",
            "Epoch [45/50], Train Loss: 0.0450, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2069, RMSE: 0.2487\n",
            "Epoch [46/50], Train Loss: 0.0450, Val Loss: 0.0632, MAPE: inf%, MAE: 0.2109, RMSE: 0.2498\n",
            "Epoch [47/50], Train Loss: 0.0449, Val Loss: 0.0631, MAPE: inf%, MAE: 0.2105, RMSE: 0.2496\n",
            "Epoch [48/50], Train Loss: 0.0453, Val Loss: 0.0637, MAPE: inf%, MAE: 0.2131, RMSE: 0.2506\n",
            "Epoch [49/50], Train Loss: 0.0455, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2063, RMSE: 0.2487\n",
            "Epoch [50/50], Train Loss: 0.0452, Val Loss: 0.0627, MAPE: inf%, MAE: 0.2057, RMSE: 0.2487\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0531, MAPE: inf%, MAE: 0.1957, RMSE: 0.2288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([20, 30])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([20, 1])) that is different to the input size (torch.Size([20, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([20, 1])) that is different to the input size (torch.Size([20, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "data = pd.read_csv('JSE_clean_truncated.csv').values  # Load your dataset\n",
        "scaler = MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(data)\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(data, input_window, horizon):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - input_window - horizon + 1):\n",
        "        x = data[i:i + input_window]\n",
        "        y = data[i + input_window + horizon - 1]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "input_window = 30\n",
        "horizon = 5\n",
        "X, y = create_sequences(normalized_data, input_window, horizon)\n",
        "\n",
        "# Train-validation-test split\n",
        "train_size = int(len(X) * 0.7)\n",
        "val_size = int(len(X) * 0.15)\n",
        "test_size = len(X) - train_size - val_size\n",
        "\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
        "X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Graph Structure (using a correlation matrix as an adjacency matrix)\n",
        "adj_matrix = np.corrcoef(normalized_data.T)  # Create adjacency matrix based on correlation\n",
        "adj_matrix = torch.tensor(adj_matrix, dtype=torch.float32)\n",
        "\n",
        "# Define the StemGNN Model\n",
        "class StemGNN(nn.Module):\n",
        "    def __init__(self, num_nodes, input_dim, hidden_dim, output_dim, adj_matrix, time_steps):\n",
        "        super(StemGNN, self).__init__()\n",
        "        self.adj_matrix = adj_matrix\n",
        "        self.time_steps = time_steps\n",
        "        self.gcn = nn.Linear(input_dim * num_nodes, hidden_dim)  # Adjusting input_dim\n",
        "        self.temporal_conv = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(hidden_dim * time_steps, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 3:\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        # Matrix multiplication with the adjacency matrix\n",
        "        x = torch.einsum('ncvl,vw->ncwl', (x, self.adj_matrix))\n",
        "\n",
        "        batch_size, num_channels, num_nodes, time_steps = x.shape\n",
        "        x = x.reshape(batch_size, -1)  # Flatten for GCN\n",
        "\n",
        "        x = torch.relu(self.gcn(x))\n",
        "\n",
        "        # Reshaping into [batch_size, hidden_dim, time_steps] for Conv1D\n",
        "        x = x.view(batch_size, 64, 1)  # Adjust to have 1 time step\n",
        "\n",
        "        x = self.temporal_conv(x)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Evaluation metrics\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    mae = nn.L1Loss()(y_true, y_pred).item()  # Mean Absolute Error\n",
        "    rmse = torch.sqrt(nn.MSELoss()(y_true, y_pred)).item()  # Root Mean Square Error\n",
        "    mape = (torch.mean(torch.abs((y_true - y_pred) / y_true)) * 100).item()  # Mean Absolute Percentage Error\n",
        "    return mape, mae, rmse\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=50):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_metrics = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        mape_total, mae_total, rmse_total = 0.0, 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_val_batch, y_val_batch in val_loader:\n",
        "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
        "                y_val_pred = model(X_val_batch)\n",
        "                loss = criterion(y_val_pred, y_val_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Compute evaluation metrics\n",
        "                mape, mae, rmse = compute_metrics(y_val_batch, y_val_pred)\n",
        "                mape_total += mape\n",
        "                mae_total += mae\n",
        "                rmse_total += rmse\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "        val_metrics.append({\n",
        "            'mape': mape_total / len(val_loader),\n",
        "            'mae': mae_total / len(val_loader),\n",
        "            'rmse': rmse_total / len(val_loader)\n",
        "        })\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
        "              f'MAPE: {mape_total/len(val_loader):.2f}%, MAE: {mae_total/len(val_loader):.4f}, RMSE: {rmse_total/len(val_loader):.4f}')\n",
        "\n",
        "    return train_losses, val_losses, val_metrics\n",
        "\n",
        "# Train the model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "num_nodes = 30\n",
        "input_dim = 30  # Based on your input\n",
        "hidden_dim = 64\n",
        "output_dim = 1\n",
        "time_steps = 1\n",
        "adj_matrix = torch.randn(num_nodes, num_nodes)  # Example adjacency matrix\n",
        "\n",
        "model = StemGNN(num_nodes, input_dim, hidden_dim, output_dim, adj_matrix.to(device), time_steps).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training the model\n",
        "train_losses, val_losses, val_metrics = train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=50)\n",
        "\n",
        "# Plot training and validation losses\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    mape_total, mae_total, rmse_total = 0.0, 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_test_batch, y_test_batch in test_loader:\n",
        "            X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
        "            y_test_pred = model(X_test_batch)\n",
        "            loss = criterion(y_test_pred, y_test_batch)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Compute evaluation metrics\n",
        "            mape, mae, rmse = compute_metrics(y_test_batch, y_test_pred)\n",
        "            mape_total += mape\n",
        "            mae_total += mae\n",
        "            rmse_total += rmse\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f'Test Loss: {test_loss:.4f}, MAPE: {mape_total/len(test_loader):.2f}%, '\n",
        "          f'MAE: {mae_total/len(test_loader):.4f}, RMSE: {rmse_total/len(test_loader):.4f}')\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate_model(model, test_loader, criterion)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "data = pd.read_csv('JSE_clean_truncated.csv').values  # Load your dataset\n",
        "scaler = MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(data)\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(data, input_window, horizon):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - input_window - horizon + 1):\n",
        "        x = data[i:i + input_window]\n",
        "        y = data[i + input_window + horizon - 1]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "# Define the StemGNN Model\n",
        "class StemGNN(nn.Module):\n",
        "    def __init__(self, num_nodes, input_dim, hidden_dim, output_dim, adj_matrix, time_steps):\n",
        "        super(StemGNN, self).__init__()\n",
        "        self.adj_matrix = adj_matrix\n",
        "        self.time_steps = time_steps\n",
        "        self.gcn = nn.Linear(input_dim * num_nodes, hidden_dim)  # Adjusting input_dim\n",
        "        self.temporal_conv = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(hidden_dim * time_steps, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 3:\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        # Matrix multiplication with the adjacency matrix\n",
        "        x = torch.einsum('ncvl,vw->ncwl', (x, self.adj_matrix))\n",
        "\n",
        "        batch_size, num_channels, num_nodes, time_steps = x.shape\n",
        "        x = x.reshape(batch_size, -1)  # Flatten for GCN\n",
        "\n",
        "        x = torch.relu(self.gcn(x))\n",
        "\n",
        "        # Reshaping into [batch_size, hidden_dim, time_steps] for Conv1D\n",
        "        x = x.view(batch_size, 64, 1)  # Adjust to have 1 time step\n",
        "\n",
        "        x = self.temporal_conv(x)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Evaluation metrics\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    mae = nn.L1Loss()(y_true, y_pred).item()  # Mean Absolute Error\n",
        "    rmse = torch.sqrt(nn.MSELoss()(y_true, y_pred)).item()  # Root Mean Square Error\n",
        "    mape = (torch.mean(torch.abs((y_true - y_pred) / (y_true + 1e-8))) * 100).item()  # Mean Absolute Percentage Error with zero handling\n",
        "    return mape, mae, rmse\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=50):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_metrics = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        mape_total, mae_total, rmse_total = 0.0, 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_val_batch, y_val_batch in val_loader:\n",
        "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
        "                y_val_pred = model(X_val_batch)\n",
        "                loss = criterion(y_val_pred, y_val_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Compute evaluation metrics\n",
        "                mape, mae, rmse = compute_metrics(y_val_batch, y_val_pred)\n",
        "                mape_total += mape\n",
        "                mae_total += mae\n",
        "                rmse_total += rmse\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "        val_metrics.append({\n",
        "            'mape': mape_total / len(val_loader),\n",
        "            'mae': mae_total / len(val_loader),\n",
        "            'rmse': rmse_total / len(val_loader)\n",
        "        })\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
        "              f'MAPE: {mape_total/len(val_loader):.2f}%, MAE: {mae_total/len(val_loader):.4f}, RMSE: {rmse_total/len(val_loader):.4f}')\n",
        "\n",
        "    return train_losses, val_losses, val_metrics\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    mape_total, mae_total, rmse_total = 0.0, 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_test_batch, y_test_batch in test_loader:\n",
        "            X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
        "            y_test_pred = model(X_test_batch)\n",
        "            loss = criterion(y_test_pred, y_test_batch)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Compute evaluation metrics\n",
        "            mape, mae, rmse = compute_metrics(y_test_batch, y_test_pred)\n",
        "            mape_total += mape\n",
        "            mae_total += mae\n",
        "            rmse_total += rmse\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f'Test Loss: {test_loss:.4f}, MAPE: {mape_total/len(test_loader):.2f}%, '\n",
        "          f'MAE: {mae_total/len(test_loader):.4f}, RMSE: {rmse_total/len(test_loader):.4f}')\n",
        "    return mape_total/len(test_loader), mae_total/len(test_loader), rmse_total/len(test_loader)\n",
        "\n",
        "# Hyperparameter Configurations\n",
        "hyperparameter_configurations = [\n",
        "    {'input_window': 30, 'output_window': 1, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "    {'input_window': 30, 'output_window': 5, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100},\n",
        "]\n",
        "\n",
        "# Run experiments\n",
        "results = []\n",
        "\n",
        "for config in hyperparameter_configurations:\n",
        "    print(f\"Training with config: {config}\")\n",
        "\n",
        "    # Create sequences based on the current configuration\n",
        "    X, y = create_sequences(normalized_data, config['input_window'], config['output_window'])\n",
        "\n",
        "    # Train-validation-test split\n",
        "    train_size = int(len(X) * 0.7)\n",
        "    val_size = int(len(X) * 0.15)\n",
        "    test_size = len(X) - train_size - val_size\n",
        "\n",
        "    X_train, y_train = X[:train_size], y[:train_size]\n",
        "    X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
        "    X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "    # Initialize model, optimizer, and criterion\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    num_nodes = X_train.shape[1]\n",
        "    input_dim = X_train.shape[2]\n",
        "    hidden_dim = config['hidden_nodes']\n",
        "    output_dim = 1\n",
        "    adj_matrix = torch.randn(num_nodes, num_nodes)  # Example adjacency matrix\n",
        "\n",
        "    riterion = nn.MSELoss()\n",
        "\n",
        "    # Train the model\n",
        "    train_losses, val_losses, val_metrics = train_model(\n",
        "        model, train_loader, val_loader, optimizer, criterion, num_epochs=config['epochs']\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    mape, mae, rmse = evaluate_model(model, test_loader, criterion)\n",
        "\n",
        "    # Store the results for this configuration\n",
        "    results.append({\n",
        "        'config': config,\n",
        "        'mape': mape,\n",
        "        'mae': mae,\n",
        "        'rmse': rmse,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'val_metrics': val_metrics\n",
        "    })\n",
        "\n",
        "# After all configurations\n",
        "for result in results:\n",
        "    config = result['config']\n",
        "    print(f\"Results for config: {config}\")\n",
        "    print(f\"Test MAPE: {result['mape']:.4f}%\")\n",
        "    print(f\"Test MAE: {result['mae']:.4f}\")\n",
        "    print(f\"Test RMSE: {result['rmse']:.4f}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "nF1x2F6_AuJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a0ec022-0fff-43ef-bded-deb98685caf3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with config: {'input_window': 30, 'output_window': 1, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32, 30])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([5, 30])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([19, 30])) that is different to the input size (torch.Size([19, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 0.0450, Val Loss: 0.0626, MAPE: 407021.17%, MAE: 0.2070, RMSE: 0.2486\n",
            "Epoch [2/100], Train Loss: 0.0450, Val Loss: 0.0626, MAPE: 407506.79%, MAE: 0.2068, RMSE: 0.2486\n",
            "Epoch [3/100], Train Loss: 0.0450, Val Loss: 0.0627, MAPE: 402652.39%, MAE: 0.2083, RMSE: 0.2488\n",
            "Epoch [4/100], Train Loss: 0.0450, Val Loss: 0.0626, MAPE: 406388.89%, MAE: 0.2071, RMSE: 0.2486\n",
            "Epoch [5/100], Train Loss: 0.0449, Val Loss: 0.0628, MAPE: 401376.63%, MAE: 0.2087, RMSE: 0.2489\n",
            "Epoch [6/100], Train Loss: 0.0449, Val Loss: 0.0628, MAPE: 399450.77%, MAE: 0.2092, RMSE: 0.2490\n",
            "Epoch [7/100], Train Loss: 0.0450, Val Loss: 0.0628, MAPE: 399506.82%, MAE: 0.2092, RMSE: 0.2490\n",
            "Epoch [8/100], Train Loss: 0.0451, Val Loss: 0.0626, MAPE: 405179.02%, MAE: 0.2074, RMSE: 0.2486\n",
            "Epoch [9/100], Train Loss: 0.0450, Val Loss: 0.0627, MAPE: 402162.10%, MAE: 0.2083, RMSE: 0.2488\n",
            "Epoch [10/100], Train Loss: 0.0449, Val Loss: 0.0627, MAPE: 403305.80%, MAE: 0.2079, RMSE: 0.2487\n",
            "Epoch [11/100], Train Loss: 0.0450, Val Loss: 0.0630, MAPE: 396523.88%, MAE: 0.2102, RMSE: 0.2493\n",
            "Epoch [12/100], Train Loss: 0.0451, Val Loss: 0.0626, MAPE: 405888.26%, MAE: 0.2071, RMSE: 0.2485\n",
            "Epoch [13/100], Train Loss: 0.0449, Val Loss: 0.0626, MAPE: 404188.56%, MAE: 0.2076, RMSE: 0.2486\n",
            "Epoch [14/100], Train Loss: 0.0450, Val Loss: 0.0626, MAPE: 407857.21%, MAE: 0.2065, RMSE: 0.2484\n",
            "Epoch [15/100], Train Loss: 0.0449, Val Loss: 0.0626, MAPE: 406545.26%, MAE: 0.2068, RMSE: 0.2485\n",
            "Epoch [16/100], Train Loss: 0.0451, Val Loss: 0.0626, MAPE: 405605.28%, MAE: 0.2071, RMSE: 0.2485\n",
            "Epoch [17/100], Train Loss: 0.0451, Val Loss: 0.0627, MAPE: 402314.54%, MAE: 0.2082, RMSE: 0.2487\n",
            "Epoch [18/100], Train Loss: 0.0450, Val Loss: 0.0628, MAPE: 399874.16%, MAE: 0.2089, RMSE: 0.2489\n",
            "Epoch [19/100], Train Loss: 0.0448, Val Loss: 0.0627, MAPE: 401817.12%, MAE: 0.2083, RMSE: 0.2487\n",
            "Epoch [20/100], Train Loss: 0.0450, Val Loss: 0.0627, MAPE: 402414.13%, MAE: 0.2081, RMSE: 0.2487\n",
            "Epoch [21/100], Train Loss: 0.0448, Val Loss: 0.0625, MAPE: 410517.52%, MAE: 0.2057, RMSE: 0.2484\n",
            "Epoch [22/100], Train Loss: 0.0449, Val Loss: 0.0627, MAPE: 400704.39%, MAE: 0.2086, RMSE: 0.2488\n",
            "Epoch [23/100], Train Loss: 0.0450, Val Loss: 0.0625, MAPE: 406517.03%, MAE: 0.2067, RMSE: 0.2484\n",
            "Epoch [24/100], Train Loss: 0.0451, Val Loss: 0.0625, MAPE: 410131.03%, MAE: 0.2057, RMSE: 0.2484\n",
            "Epoch [25/100], Train Loss: 0.0450, Val Loss: 0.0629, MAPE: 397364.92%, MAE: 0.2096, RMSE: 0.2491\n",
            "Epoch [26/100], Train Loss: 0.0450, Val Loss: 0.0626, MAPE: 404125.22%, MAE: 0.2074, RMSE: 0.2485\n",
            "Epoch [27/100], Train Loss: 0.0450, Val Loss: 0.0626, MAPE: 405525.06%, MAE: 0.2069, RMSE: 0.2484\n",
            "Epoch [28/100], Train Loss: 0.0449, Val Loss: 0.0625, MAPE: 411353.31%, MAE: 0.2052, RMSE: 0.2483\n",
            "Epoch [29/100], Train Loss: 0.0449, Val Loss: 0.0625, MAPE: 406734.45%, MAE: 0.2066, RMSE: 0.2484\n",
            "Epoch [30/100], Train Loss: 0.0449, Val Loss: 0.0626, MAPE: 402042.99%, MAE: 0.2080, RMSE: 0.2486\n",
            "Epoch [31/100], Train Loss: 0.0451, Val Loss: 0.0627, MAPE: 400654.23%, MAE: 0.2085, RMSE: 0.2487\n",
            "Epoch [32/100], Train Loss: 0.0449, Val Loss: 0.0625, MAPE: 409242.70%, MAE: 0.2058, RMSE: 0.2483\n",
            "Epoch [33/100], Train Loss: 0.0451, Val Loss: 0.0630, MAPE: 395186.66%, MAE: 0.2104, RMSE: 0.2493\n",
            "Epoch [34/100], Train Loss: 0.0449, Val Loss: 0.0626, MAPE: 402045.34%, MAE: 0.2080, RMSE: 0.2486\n",
            "Epoch [35/100], Train Loss: 0.0452, Val Loss: 0.0626, MAPE: 404672.25%, MAE: 0.2072, RMSE: 0.2485\n",
            "Epoch [36/100], Train Loss: 0.0451, Val Loss: 0.0625, MAPE: 407512.83%, MAE: 0.2063, RMSE: 0.2484\n",
            "Epoch [37/100], Train Loss: 0.0452, Val Loss: 0.0625, MAPE: 406259.97%, MAE: 0.2067, RMSE: 0.2484\n",
            "Epoch [38/100], Train Loss: 0.0449, Val Loss: 0.0626, MAPE: 402057.07%, MAE: 0.2079, RMSE: 0.2486\n",
            "Epoch [39/100], Train Loss: 0.0450, Val Loss: 0.0626, MAPE: 404417.58%, MAE: 0.2072, RMSE: 0.2485\n",
            "Epoch [40/100], Train Loss: 0.0449, Val Loss: 0.0629, MAPE: 396232.92%, MAE: 0.2098, RMSE: 0.2491\n",
            "Epoch [41/100], Train Loss: 0.0451, Val Loss: 0.0629, MAPE: 396099.27%, MAE: 0.2099, RMSE: 0.2492\n",
            "Epoch [42/100], Train Loss: 0.0450, Val Loss: 0.0625, MAPE: 405162.61%, MAE: 0.2069, RMSE: 0.2484\n",
            "Epoch [43/100], Train Loss: 0.0450, Val Loss: 0.0625, MAPE: 404340.25%, MAE: 0.2071, RMSE: 0.2484\n",
            "Epoch [44/100], Train Loss: 0.0449, Val Loss: 0.0626, MAPE: 404525.12%, MAE: 0.2070, RMSE: 0.2485\n",
            "Epoch [45/100], Train Loss: 0.0449, Val Loss: 0.0631, MAPE: 393156.71%, MAE: 0.2109, RMSE: 0.2495\n",
            "Epoch [46/100], Train Loss: 0.0452, Val Loss: 0.0625, MAPE: 406031.19%, MAE: 0.2066, RMSE: 0.2484\n",
            "Epoch [47/100], Train Loss: 0.0450, Val Loss: 0.0626, MAPE: 401623.10%, MAE: 0.2079, RMSE: 0.2486\n",
            "Epoch [48/100], Train Loss: 0.0450, Val Loss: 0.0625, MAPE: 405116.12%, MAE: 0.2068, RMSE: 0.2484\n",
            "Epoch [49/100], Train Loss: 0.0449, Val Loss: 0.0627, MAPE: 398792.29%, MAE: 0.2088, RMSE: 0.2488\n",
            "Epoch [50/100], Train Loss: 0.0448, Val Loss: 0.0625, MAPE: 406675.08%, MAE: 0.2064, RMSE: 0.2484\n",
            "Epoch [51/100], Train Loss: 0.0451, Val Loss: 0.0626, MAPE: 403176.78%, MAE: 0.2074, RMSE: 0.2485\n",
            "Epoch [52/100], Train Loss: 0.0451, Val Loss: 0.0627, MAPE: 399179.41%, MAE: 0.2088, RMSE: 0.2488\n",
            "Epoch [53/100], Train Loss: 0.0449, Val Loss: 0.0626, MAPE: 403423.72%, MAE: 0.2074, RMSE: 0.2485\n",
            "Epoch [54/100], Train Loss: 0.0450, Val Loss: 0.0625, MAPE: 405606.30%, MAE: 0.2066, RMSE: 0.2484\n",
            "Epoch [55/100], Train Loss: 0.0449, Val Loss: 0.0625, MAPE: 405627.28%, MAE: 0.2067, RMSE: 0.2484\n",
            "Epoch [56/100], Train Loss: 0.0450, Val Loss: 0.0625, MAPE: 405479.82%, MAE: 0.2067, RMSE: 0.2484\n",
            "Epoch [57/100], Train Loss: 0.0451, Val Loss: 0.0625, MAPE: 405054.33%, MAE: 0.2068, RMSE: 0.2484\n",
            "Epoch [58/100], Train Loss: 0.0451, Val Loss: 0.0626, MAPE: 402219.58%, MAE: 0.2076, RMSE: 0.2485\n",
            "Epoch [59/100], Train Loss: 0.0451, Val Loss: 0.0627, MAPE: 399839.45%, MAE: 0.2085, RMSE: 0.2487\n",
            "Epoch [60/100], Train Loss: 0.0450, Val Loss: 0.0625, MAPE: 411048.77%, MAE: 0.2050, RMSE: 0.2483\n",
            "Epoch [61/100], Train Loss: 0.0451, Val Loss: 0.0625, MAPE: 412527.12%, MAE: 0.2046, RMSE: 0.2483\n",
            "Epoch [62/100], Train Loss: 0.0450, Val Loss: 0.0626, MAPE: 402705.96%, MAE: 0.2075, RMSE: 0.2485\n",
            "Epoch [63/100], Train Loss: 0.0452, Val Loss: 0.0628, MAPE: 397370.84%, MAE: 0.2093, RMSE: 0.2489\n",
            "Epoch [64/100], Train Loss: 0.0450, Val Loss: 0.0629, MAPE: 395290.03%, MAE: 0.2099, RMSE: 0.2491\n",
            "Epoch [65/100], Train Loss: 0.0452, Val Loss: 0.0626, MAPE: 401898.98%, MAE: 0.2078, RMSE: 0.2486\n",
            "Epoch [66/100], Train Loss: 0.0450, Val Loss: 0.0626, MAPE: 402528.57%, MAE: 0.2076, RMSE: 0.2485\n",
            "Epoch [67/100], Train Loss: 0.0448, Val Loss: 0.0625, MAPE: 405986.89%, MAE: 0.2065, RMSE: 0.2484\n",
            "Epoch [68/100], Train Loss: 0.0449, Val Loss: 0.0625, MAPE: 410466.37%, MAE: 0.2052, RMSE: 0.2483\n",
            "Epoch [69/100], Train Loss: 0.0449, Val Loss: 0.0625, MAPE: 409000.42%, MAE: 0.2056, RMSE: 0.2483\n",
            "Epoch [70/100], Train Loss: 0.0452, Val Loss: 0.0626, MAPE: 402168.47%, MAE: 0.2077, RMSE: 0.2485\n",
            "Epoch [71/100], Train Loss: 0.0451, Val Loss: 0.0625, MAPE: 408099.36%, MAE: 0.2059, RMSE: 0.2483\n",
            "Epoch [72/100], Train Loss: 0.0450, Val Loss: 0.0627, MAPE: 400239.33%, MAE: 0.2083, RMSE: 0.2487\n",
            "Epoch [73/100], Train Loss: 0.0449, Val Loss: 0.0626, MAPE: 400808.94%, MAE: 0.2080, RMSE: 0.2486\n",
            "Epoch [74/100], Train Loss: 0.0449, Val Loss: 0.0625, MAPE: 405754.86%, MAE: 0.2065, RMSE: 0.2484\n",
            "Epoch [75/100], Train Loss: 0.0450, Val Loss: 0.0625, MAPE: 412427.75%, MAE: 0.2046, RMSE: 0.2483\n",
            "Epoch [76/100], Train Loss: 0.0451, Val Loss: 0.0625, MAPE: 408743.58%, MAE: 0.2057, RMSE: 0.2483\n",
            "Epoch [77/100], Train Loss: 0.0453, Val Loss: 0.0626, MAPE: 401242.47%, MAE: 0.2079, RMSE: 0.2486\n",
            "Epoch [78/100], Train Loss: 0.0451, Val Loss: 0.0625, MAPE: 406570.75%, MAE: 0.2063, RMSE: 0.2483\n",
            "Epoch [79/100], Train Loss: 0.0449, Val Loss: 0.0626, MAPE: 402437.89%, MAE: 0.2075, RMSE: 0.2485\n",
            "Epoch [80/100], Train Loss: 0.0451, Val Loss: 0.0625, MAPE: 408562.61%, MAE: 0.2057, RMSE: 0.2483\n",
            "Epoch [81/100], Train Loss: 0.0449, Val Loss: 0.0625, MAPE: 406272.92%, MAE: 0.2064, RMSE: 0.2484\n",
            "Epoch [82/100], Train Loss: 0.0450, Val Loss: 0.0625, MAPE: 404576.23%, MAE: 0.2069, RMSE: 0.2484\n",
            "Epoch [83/100], Train Loss: 0.0449, Val Loss: 0.0625, MAPE: 406970.41%, MAE: 0.2062, RMSE: 0.2483\n",
            "Epoch [84/100], Train Loss: 0.0450, Val Loss: 0.0626, MAPE: 402205.53%, MAE: 0.2076, RMSE: 0.2485\n",
            "Epoch [85/100], Train Loss: 0.0449, Val Loss: 0.0625, MAPE: 404992.44%, MAE: 0.2067, RMSE: 0.2484\n",
            "Epoch [86/100], Train Loss: 0.0451, Val Loss: 0.0625, MAPE: 406448.80%, MAE: 0.2063, RMSE: 0.2483\n",
            "Epoch [87/100], Train Loss: 0.0448, Val Loss: 0.0625, MAPE: 412371.20%, MAE: 0.2046, RMSE: 0.2483\n",
            "Epoch [88/100], Train Loss: 0.0450, Val Loss: 0.0626, MAPE: 401943.05%, MAE: 0.2077, RMSE: 0.2485\n",
            "Epoch [89/100], Train Loss: 0.0450, Val Loss: 0.0626, MAPE: 402815.75%, MAE: 0.2073, RMSE: 0.2485\n",
            "Epoch [90/100], Train Loss: 0.0449, Val Loss: 0.0628, MAPE: 397109.57%, MAE: 0.2092, RMSE: 0.2489\n",
            "Epoch [91/100], Train Loss: 0.0449, Val Loss: 0.0625, MAPE: 406309.70%, MAE: 0.2062, RMSE: 0.2483\n",
            "Epoch [92/100], Train Loss: 0.0451, Val Loss: 0.0626, MAPE: 401641.56%, MAE: 0.2077, RMSE: 0.2485\n",
            "Epoch [93/100], Train Loss: 0.0452, Val Loss: 0.0625, MAPE: 405854.25%, MAE: 0.2064, RMSE: 0.2483\n",
            "Epoch [94/100], Train Loss: 0.0451, Val Loss: 0.0625, MAPE: 413149.15%, MAE: 0.2043, RMSE: 0.2484\n",
            "Epoch [95/100], Train Loss: 0.0452, Val Loss: 0.0626, MAPE: 399782.86%, MAE: 0.2082, RMSE: 0.2486\n",
            "Epoch [96/100], Train Loss: 0.0452, Val Loss: 0.0625, MAPE: 403457.67%, MAE: 0.2070, RMSE: 0.2484\n",
            "Epoch [97/100], Train Loss: 0.0450, Val Loss: 0.0625, MAPE: 405664.93%, MAE: 0.2064, RMSE: 0.2483\n",
            "Epoch [98/100], Train Loss: 0.0451, Val Loss: 0.0625, MAPE: 411205.69%, MAE: 0.2048, RMSE: 0.2483\n",
            "Epoch [99/100], Train Loss: 0.0450, Val Loss: 0.0625, MAPE: 409125.89%, MAE: 0.2054, RMSE: 0.2483\n",
            "Epoch [100/100], Train Loss: 0.0450, Val Loss: 0.0625, MAPE: 409109.28%, MAE: 0.2054, RMSE: 0.2483\n",
            "Test Loss: 0.0524, MAPE: 1595443.93%, MAE: 0.1919, RMSE: 0.2272\n",
            "Training with config: {'input_window': 30, 'output_window': 5, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([20, 30])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([20, 1])) that is different to the input size (torch.Size([20, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([20, 1])) that is different to the input size (torch.Size([20, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([2, 30])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([18, 30])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([18, 1])) that is different to the input size (torch.Size([18, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([18, 1])) that is different to the input size (torch.Size([18, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 0.0452, Val Loss: 0.0627, MAPE: 413584.44%, MAE: 0.2051, RMSE: 0.2486\n",
            "Epoch [2/100], Train Loss: 0.0452, Val Loss: 0.0628, MAPE: 403912.50%, MAE: 0.2081, RMSE: 0.2489\n",
            "Epoch [3/100], Train Loss: 0.0449, Val Loss: 0.0627, MAPE: 412876.93%, MAE: 0.2054, RMSE: 0.2487\n",
            "Epoch [4/100], Train Loss: 0.0453, Val Loss: 0.0628, MAPE: 403191.97%, MAE: 0.2084, RMSE: 0.2490\n",
            "Epoch [5/100], Train Loss: 0.0451, Val Loss: 0.0629, MAPE: 401118.78%, MAE: 0.2091, RMSE: 0.2492\n",
            "Epoch [6/100], Train Loss: 0.0452, Val Loss: 0.0632, MAPE: 396687.76%, MAE: 0.2107, RMSE: 0.2497\n",
            "Epoch [7/100], Train Loss: 0.0451, Val Loss: 0.0629, MAPE: 401110.07%, MAE: 0.2093, RMSE: 0.2492\n",
            "Epoch [8/100], Train Loss: 0.0454, Val Loss: 0.0628, MAPE: 404095.55%, MAE: 0.2083, RMSE: 0.2490\n",
            "Epoch [9/100], Train Loss: 0.0455, Val Loss: 0.0627, MAPE: 413859.29%, MAE: 0.2053, RMSE: 0.2487\n",
            "Epoch [10/100], Train Loss: 0.0450, Val Loss: 0.0628, MAPE: 405327.97%, MAE: 0.2079, RMSE: 0.2489\n",
            "Epoch [11/100], Train Loss: 0.0452, Val Loss: 0.0629, MAPE: 403656.49%, MAE: 0.2086, RMSE: 0.2491\n",
            "Epoch [12/100], Train Loss: 0.0453, Val Loss: 0.0628, MAPE: 419423.49%, MAE: 0.2039, RMSE: 0.2489\n",
            "Epoch [13/100], Train Loss: 0.0451, Val Loss: 0.0628, MAPE: 405560.13%, MAE: 0.2080, RMSE: 0.2490\n",
            "Epoch [14/100], Train Loss: 0.0453, Val Loss: 0.0628, MAPE: 405320.27%, MAE: 0.2081, RMSE: 0.2490\n",
            "Epoch [15/100], Train Loss: 0.0449, Val Loss: 0.0627, MAPE: 410244.53%, MAE: 0.2065, RMSE: 0.2488\n",
            "Epoch [16/100], Train Loss: 0.0451, Val Loss: 0.0632, MAPE: 397417.02%, MAE: 0.2109, RMSE: 0.2498\n",
            "Epoch [17/100], Train Loss: 0.0453, Val Loss: 0.0629, MAPE: 423526.63%, MAE: 0.2030, RMSE: 0.2491\n",
            "Epoch [18/100], Train Loss: 0.0453, Val Loss: 0.0632, MAPE: 397451.94%, MAE: 0.2108, RMSE: 0.2498\n",
            "Epoch [19/100], Train Loss: 0.0451, Val Loss: 0.0629, MAPE: 402719.00%, MAE: 0.2089, RMSE: 0.2492\n",
            "Epoch [20/100], Train Loss: 0.0450, Val Loss: 0.0628, MAPE: 405865.22%, MAE: 0.2079, RMSE: 0.2490\n",
            "Epoch [21/100], Train Loss: 0.0453, Val Loss: 0.0629, MAPE: 404319.02%, MAE: 0.2084, RMSE: 0.2491\n",
            "Epoch [22/100], Train Loss: 0.0452, Val Loss: 0.0627, MAPE: 411794.73%, MAE: 0.2062, RMSE: 0.2488\n",
            "Epoch [23/100], Train Loss: 0.0451, Val Loss: 0.0633, MAPE: 397393.16%, MAE: 0.2110, RMSE: 0.2499\n",
            "Epoch [24/100], Train Loss: 0.0450, Val Loss: 0.0627, MAPE: 414118.05%, MAE: 0.2055, RMSE: 0.2488\n",
            "Epoch [25/100], Train Loss: 0.0449, Val Loss: 0.0628, MAPE: 407353.71%, MAE: 0.2076, RMSE: 0.2489\n",
            "Epoch [26/100], Train Loss: 0.0450, Val Loss: 0.0630, MAPE: 401793.08%, MAE: 0.2096, RMSE: 0.2494\n",
            "Epoch [27/100], Train Loss: 0.0453, Val Loss: 0.0635, MAPE: 394041.01%, MAE: 0.2123, RMSE: 0.2503\n",
            "Epoch [28/100], Train Loss: 0.0452, Val Loss: 0.0630, MAPE: 403200.98%, MAE: 0.2090, RMSE: 0.2493\n",
            "Epoch [29/100], Train Loss: 0.0450, Val Loss: 0.0628, MAPE: 408626.84%, MAE: 0.2074, RMSE: 0.2489\n",
            "Epoch [30/100], Train Loss: 0.0450, Val Loss: 0.0628, MAPE: 409231.20%, MAE: 0.2072, RMSE: 0.2489\n",
            "Epoch [31/100], Train Loss: 0.0451, Val Loss: 0.0630, MAPE: 403250.48%, MAE: 0.2093, RMSE: 0.2494\n",
            "Epoch [32/100], Train Loss: 0.0454, Val Loss: 0.0629, MAPE: 408363.61%, MAE: 0.2077, RMSE: 0.2490\n",
            "Epoch [33/100], Train Loss: 0.0454, Val Loss: 0.0630, MAPE: 425680.05%, MAE: 0.2028, RMSE: 0.2493\n",
            "Epoch [34/100], Train Loss: 0.0454, Val Loss: 0.0628, MAPE: 410193.35%, MAE: 0.2070, RMSE: 0.2489\n",
            "Epoch [35/100], Train Loss: 0.0448, Val Loss: 0.0629, MAPE: 408276.89%, MAE: 0.2077, RMSE: 0.2490\n",
            "Epoch [36/100], Train Loss: 0.0452, Val Loss: 0.0644, MAPE: 386128.16%, MAE: 0.2159, RMSE: 0.2521\n",
            "Epoch [37/100], Train Loss: 0.0452, Val Loss: 0.0628, MAPE: 418247.57%, MAE: 0.2047, RMSE: 0.2489\n",
            "Epoch [38/100], Train Loss: 0.0451, Val Loss: 0.0628, MAPE: 409680.35%, MAE: 0.2072, RMSE: 0.2490\n",
            "Epoch [39/100], Train Loss: 0.0451, Val Loss: 0.0628, MAPE: 413951.65%, MAE: 0.2060, RMSE: 0.2489\n",
            "Epoch [40/100], Train Loss: 0.0453, Val Loss: 0.0629, MAPE: 422183.34%, MAE: 0.2037, RMSE: 0.2490\n",
            "Epoch [41/100], Train Loss: 0.0452, Val Loss: 0.0630, MAPE: 405089.69%, MAE: 0.2088, RMSE: 0.2493\n",
            "Epoch [42/100], Train Loss: 0.0451, Val Loss: 0.0629, MAPE: 409011.37%, MAE: 0.2076, RMSE: 0.2491\n",
            "Epoch [43/100], Train Loss: 0.0451, Val Loss: 0.0632, MAPE: 402016.76%, MAE: 0.2100, RMSE: 0.2497\n",
            "Epoch [44/100], Train Loss: 0.0450, Val Loss: 0.0630, MAPE: 406009.80%, MAE: 0.2086, RMSE: 0.2493\n",
            "Epoch [45/100], Train Loss: 0.0453, Val Loss: 0.0628, MAPE: 413459.99%, MAE: 0.2063, RMSE: 0.2489\n",
            "Epoch [46/100], Train Loss: 0.0449, Val Loss: 0.0628, MAPE: 411708.20%, MAE: 0.2068, RMSE: 0.2490\n",
            "Epoch [47/100], Train Loss: 0.0451, Val Loss: 0.0628, MAPE: 412457.03%, MAE: 0.2064, RMSE: 0.2489\n",
            "Epoch [48/100], Train Loss: 0.0452, Val Loss: 0.0628, MAPE: 416496.01%, MAE: 0.2054, RMSE: 0.2489\n",
            "Epoch [49/100], Train Loss: 0.0451, Val Loss: 0.0629, MAPE: 408824.91%, MAE: 0.2076, RMSE: 0.2490\n",
            "Epoch [50/100], Train Loss: 0.0450, Val Loss: 0.0633, MAPE: 399184.60%, MAE: 0.2110, RMSE: 0.2500\n",
            "Epoch [51/100], Train Loss: 0.0451, Val Loss: 0.0628, MAPE: 414113.64%, MAE: 0.2061, RMSE: 0.2489\n",
            "Epoch [52/100], Train Loss: 0.0449, Val Loss: 0.0629, MAPE: 409747.78%, MAE: 0.2075, RMSE: 0.2491\n",
            "Epoch [53/100], Train Loss: 0.0452, Val Loss: 0.0628, MAPE: 412634.14%, MAE: 0.2066, RMSE: 0.2490\n",
            "Epoch [54/100], Train Loss: 0.0453, Val Loss: 0.0628, MAPE: 414955.52%, MAE: 0.2059, RMSE: 0.2489\n",
            "Epoch [55/100], Train Loss: 0.0452, Val Loss: 0.0633, MAPE: 399611.17%, MAE: 0.2111, RMSE: 0.2500\n",
            "Epoch [56/100], Train Loss: 0.0452, Val Loss: 0.0630, MAPE: 405457.69%, MAE: 0.2090, RMSE: 0.2494\n",
            "Epoch [57/100], Train Loss: 0.0455, Val Loss: 0.0628, MAPE: 418442.09%, MAE: 0.2051, RMSE: 0.2490\n",
            "Epoch [58/100], Train Loss: 0.0451, Val Loss: 0.0632, MAPE: 403558.40%, MAE: 0.2098, RMSE: 0.2497\n",
            "Epoch [59/100], Train Loss: 0.0452, Val Loss: 0.0631, MAPE: 403478.17%, MAE: 0.2098, RMSE: 0.2496\n",
            "Epoch [60/100], Train Loss: 0.0451, Val Loss: 0.0629, MAPE: 412900.65%, MAE: 0.2068, RMSE: 0.2490\n",
            "Epoch [61/100], Train Loss: 0.0451, Val Loss: 0.0630, MAPE: 408093.74%, MAE: 0.2082, RMSE: 0.2492\n",
            "Epoch [62/100], Train Loss: 0.0452, Val Loss: 0.0628, MAPE: 415647.56%, MAE: 0.2059, RMSE: 0.2490\n",
            "Epoch [63/100], Train Loss: 0.0454, Val Loss: 0.0630, MAPE: 407649.08%, MAE: 0.2085, RMSE: 0.2494\n",
            "Epoch [64/100], Train Loss: 0.0452, Val Loss: 0.0629, MAPE: 409220.69%, MAE: 0.2078, RMSE: 0.2492\n",
            "Epoch [65/100], Train Loss: 0.0453, Val Loss: 0.0631, MAPE: 404592.13%, MAE: 0.2094, RMSE: 0.2495\n",
            "Epoch [66/100], Train Loss: 0.0450, Val Loss: 0.0632, MAPE: 401952.26%, MAE: 0.2104, RMSE: 0.2498\n",
            "Epoch [67/100], Train Loss: 0.0455, Val Loss: 0.0629, MAPE: 412519.43%, MAE: 0.2068, RMSE: 0.2490\n",
            "Epoch [68/100], Train Loss: 0.0453, Val Loss: 0.0628, MAPE: 418512.69%, MAE: 0.2051, RMSE: 0.2490\n",
            "Epoch [69/100], Train Loss: 0.0449, Val Loss: 0.0631, MAPE: 406010.97%, MAE: 0.2090, RMSE: 0.2495\n",
            "Epoch [70/100], Train Loss: 0.0451, Val Loss: 0.0629, MAPE: 408721.10%, MAE: 0.2081, RMSE: 0.2492\n",
            "Epoch [71/100], Train Loss: 0.0451, Val Loss: 0.0630, MAPE: 407668.76%, MAE: 0.2085, RMSE: 0.2493\n",
            "Epoch [72/100], Train Loss: 0.0449, Val Loss: 0.0629, MAPE: 411054.44%, MAE: 0.2075, RMSE: 0.2492\n",
            "Epoch [73/100], Train Loss: 0.0453, Val Loss: 0.0629, MAPE: 411902.70%, MAE: 0.2073, RMSE: 0.2492\n",
            "Epoch [74/100], Train Loss: 0.0452, Val Loss: 0.0628, MAPE: 416400.23%, MAE: 0.2059, RMSE: 0.2490\n",
            "Epoch [75/100], Train Loss: 0.0457, Val Loss: 0.0633, MAPE: 403307.30%, MAE: 0.2103, RMSE: 0.2499\n",
            "Epoch [76/100], Train Loss: 0.0455, Val Loss: 0.0628, MAPE: 416464.04%, MAE: 0.2060, RMSE: 0.2490\n",
            "Epoch [77/100], Train Loss: 0.0450, Val Loss: 0.0629, MAPE: 413962.30%, MAE: 0.2067, RMSE: 0.2491\n",
            "Epoch [78/100], Train Loss: 0.0449, Val Loss: 0.0631, MAPE: 405969.39%, MAE: 0.2094, RMSE: 0.2496\n",
            "Epoch [79/100], Train Loss: 0.0450, Val Loss: 0.0629, MAPE: 412180.61%, MAE: 0.2073, RMSE: 0.2491\n",
            "Epoch [80/100], Train Loss: 0.0454, Val Loss: 0.0636, MAPE: 399057.33%, MAE: 0.2120, RMSE: 0.2505\n",
            "Epoch [81/100], Train Loss: 0.0453, Val Loss: 0.0632, MAPE: 405740.03%, MAE: 0.2095, RMSE: 0.2497\n",
            "Epoch [82/100], Train Loss: 0.0453, Val Loss: 0.0629, MAPE: 416391.19%, MAE: 0.2062, RMSE: 0.2491\n",
            "Epoch [83/100], Train Loss: 0.0449, Val Loss: 0.0630, MAPE: 410917.81%, MAE: 0.2079, RMSE: 0.2493\n",
            "Epoch [84/100], Train Loss: 0.0456, Val Loss: 0.0634, MAPE: 401245.88%, MAE: 0.2112, RMSE: 0.2502\n",
            "Epoch [85/100], Train Loss: 0.0449, Val Loss: 0.0631, MAPE: 408416.70%, MAE: 0.2089, RMSE: 0.2495\n",
            "Epoch [86/100], Train Loss: 0.0449, Val Loss: 0.0634, MAPE: 402621.93%, MAE: 0.2109, RMSE: 0.2501\n",
            "Epoch [87/100], Train Loss: 0.0450, Val Loss: 0.0634, MAPE: 402980.11%, MAE: 0.2108, RMSE: 0.2501\n",
            "Epoch [88/100], Train Loss: 0.0453, Val Loss: 0.0631, MAPE: 407982.85%, MAE: 0.2090, RMSE: 0.2495\n",
            "Epoch [89/100], Train Loss: 0.0450, Val Loss: 0.0629, MAPE: 416056.80%, MAE: 0.2064, RMSE: 0.2491\n",
            "Epoch [90/100], Train Loss: 0.0449, Val Loss: 0.0631, MAPE: 408981.39%, MAE: 0.2087, RMSE: 0.2495\n",
            "Epoch [91/100], Train Loss: 0.0450, Val Loss: 0.0629, MAPE: 417018.42%, MAE: 0.2061, RMSE: 0.2491\n",
            "Epoch [92/100], Train Loss: 0.0453, Val Loss: 0.0632, MAPE: 405398.91%, MAE: 0.2099, RMSE: 0.2498\n",
            "Epoch [93/100], Train Loss: 0.0451, Val Loss: 0.0633, MAPE: 403566.42%, MAE: 0.2106, RMSE: 0.2500\n",
            "Epoch [94/100], Train Loss: 0.0452, Val Loss: 0.0638, MAPE: 397473.64%, MAE: 0.2130, RMSE: 0.2510\n",
            "Epoch [95/100], Train Loss: 0.0449, Val Loss: 0.0635, MAPE: 401868.52%, MAE: 0.2112, RMSE: 0.2503\n",
            "Epoch [96/100], Train Loss: 0.0453, Val Loss: 0.0641, MAPE: 394389.61%, MAE: 0.2141, RMSE: 0.2515\n",
            "Epoch [97/100], Train Loss: 0.0450, Val Loss: 0.0634, MAPE: 403111.59%, MAE: 0.2107, RMSE: 0.2501\n",
            "Epoch [98/100], Train Loss: 0.0448, Val Loss: 0.0636, MAPE: 400172.83%, MAE: 0.2120, RMSE: 0.2506\n",
            "Epoch [99/100], Train Loss: 0.0452, Val Loss: 0.0635, MAPE: 401912.02%, MAE: 0.2114, RMSE: 0.2504\n",
            "Epoch [100/100], Train Loss: 0.0449, Val Loss: 0.0630, MAPE: 413315.02%, MAE: 0.2074, RMSE: 0.2492\n",
            "Test Loss: 0.0537, MAPE: 1890667.92%, MAE: 0.1935, RMSE: 0.2300\n",
            "Results for config: {'input_window': 30, 'output_window': 1, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100}\n",
            "Test MAPE: 1595443.9327%\n",
            "Test MAE: 0.1919\n",
            "Test RMSE: 0.2272\n",
            "--------------------------------------------------\n",
            "Results for config: {'input_window': 30, 'output_window': 5, 'hidden_nodes': 64, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100}\n",
            "Test MAPE: 1890667.9176%\n",
            "Test MAE: 0.1935\n",
            "Test RMSE: 0.2300\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hte2WnQ5Fi8u"
      }
    }
  ]
}